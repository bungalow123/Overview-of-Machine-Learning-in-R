---
title: "Overview of Some Machine Learning Methods in R"
author: "Eddie Liu"
date: "November 9, 2019"
output: html_document
---

#Bias-Variance
####- Higher variance, lower bias
####- (Non-linear) Training data lowest to highest MSE: High, Medium, Linear (variability)
####- (Non-linear) Testing data lowest to highest MSE: Medium, High, Linear (variability) 
####- Small MSE_train, Large MSE_test = overfitting
####- **Testing MSE is larger than Training MSE regardless of model**
####- **Bias decreases more rapidly than variance increases until a certain point** Exception: F is linear (then bias doesn't decrease as variance increases)

#K-Nearest Neighbor
####- A training dataframe/ matrix with only predictors
####- A vector of the training categorical response variable
####- A testing dataframe with only predictors

```{r}
library(class)
set.seed(123)
data <- read.delim('banknote.txt')

#Sample 130 values for training data
i.train <- sample(1:nrow(data), 130, replace = F)

#Training dataset
#Testing dataset
#Remove response column
data.train <- data[i.train, -7]
data.test <- data[-i.train, -7]

#Vector of training response (Counterfeit in this case). Vector of testing response. Y = response column
counterfeit.train <- data[i.train, ]$Y
counterfeit.test <- data[-i.train, ]$Y
#Or
counterfeit.train <- data$Y[i.train]
counterfeit.test <- data$Y[-i.train]

#K = 1
m.knn <- knn(data.train, data.test, counterfeit.train, k=1)

table(m.knn, counterfeit.test)

#K = 2
m.knn <- knn(data.train, data.test, counterfeit.train, k=2)

table(m.knn, counterfeit.test)
```

#####Training error rate for k = 2 is 1/70

```{r}
#Compute misclassification error rate for k = 1:20
#Graph looks strange here because we have a variable (Diagonal) that does a very good job of classification
avg.error <- c()
for(k in 1:20){
  predictions <- knn(data.train, data.test, counterfeit.train, k)
  avg.error[k] <- mean(predictions != counterfeit.test)
}

plot(1:20, avg.error)



#Standardize predictors
library(ISLR)
dim(Caravan)

#Standardize predictors but not response
X <- Caravan[ ,-86] 
X.standard <- scale(X)

set.seed(123)
test.i <- sample(1:nrow(Caravan), 1000, replace=F)

X.standard.test <- X.standard[test.i, ]
X.standard.train <- X.standard[-test.i, ]
Y.test <- Caravan$Purchase[test.i]
Y.train <- Caravan$Purchase[-test.i]

#K = 2
m.knn <- knn(X.standard.train, X.standard.test, Y.train, k=2)

table(m.knn, Y.test)
```


###Conceptual
####- Higher K, less flexibility, greater bias
####- KNN uses Euclidean distance and ignores units
####- Standardize predictor variables
####2nd example see Lecture 2.2


#Logistic Regression
####- Used for binary response variable
####- Use log-odds because regular response variable Y is not linearly associated with x (but log-odds ratio is)
####- Coefficient interpretation: a one-unit difference in x is associated with a multiplicative change in the odds of e^beta1
```{r}
data <- read.delim('banknote.txt')

#m1 estimates probability a banknote is counterfeit using the "Diagonal" variable, assuming 50-50 chance (0.5)
m1 <- glm(Y~Diagonal, data=data, family=binomial)
summary(m1)

#predict() defaults to predicted log-odds
probs <- predict(m1, type="response")
plot(probs~data$Diagonal)

#m2 estimates probability a banknote is counterfeit using the "Left" variable, assuming 50-50 chance (0.5)
m2 <- glm(Y~Left, data=data, family=binomial)
probs <- predict(m2, type="response")
classification <- rep(0,200)
classification[probs > 0.5] <- 1
table(classification, data$Y)
```

####- Can 'tune' the classifications by changing the probability threshold required to classify


#Linear Discriminant Analysis (LDA)
####- Algorithm for implementing Bayes Classifier
####- Finds best place to make the 'cut' to classify observations as belonging to 1 of 2 distributions
####- Mislassification probability lowest for a linear split 
####- A greater number of parameters is associated with a model with higher variance and lower bias
####- In theory, when all parameters are known and the model is correctly specified, this is the best approach to minimize testing error rate
#Bayes Classifier
####- Most optimal if we do not have to estimate any quantities
####- Assume distribution is gaussian for each category
####- Assume standard deviation is the same for each category
####- Assume prior probabilities of belonging to a category are equal 
####- LDA estimates group means for Bayes
```{r}
library(MASS)
set.seed(123)

data <- read.delim('banknote.txt')

#Split evenly
i.train <- sample(1:nrow(data), nrow(data)/2, replace=FALSE)
data.train <- data[i.train, ]
data.test <- data[-i.train, ]

#LDA model
lda.1 <- lda(Y~Diagonal, data=data.train)
lda.1

#Make predictions with training data
predictions.train <- predict(lda.1)
table(predictions.train$class, data.train$Y)

#Evaluate predictive success on Testing Data
predictions.test <- predict(lda.1, newdata=data.test)
table(predictions.test$class, data.test$Y)
```



#Quadratic Discriminant Analysis (QDA)
####- If we instead assumed that each group had its own variance and covariance terms
```{r}
library(MASS)
set.seed(123)

#QDA model
qda.1 <- qda(Y~Top + Bottom, data=data.train)
qda.1

#Make predictions with training data
predictions.train <- predict(qda.1)
table(predictions.train$class, data.train$Y)
```

#Comparisons Between the Above Methods
##LDA vs. Logistic
####- **Both assume a linear boundary between the 2 groups**
####- LDA goes further and assumes predictors follow a Normal distribution in both groups
####- If there truly is a linear boundary then both methods do well
####- If the distributions are truly normal, LDA will do well. Otherwise, logistic is likely to do better

##LDA vs. QDA
####- **QDA allows nonlinear boundary**
####- **LDA assumes variance/ correlation within each group is same across groups**
####- **QDA allows for different correlation structures within each group**
####- QDA requires separate covariance estimations for each category
####- QDA is much more flexible: so variability increases. Unless bias decreases, error rate will increase 

##LDA vs. KNN
####- **KNN makes no assumptions about the distributions**
####- KNN is non-parametric (doesn't assume prior distribution) and so is much more flexible than LDA

##LDA/ QDA vs. KNN
####- **KNN will do better with highly non-linear boundary**
####- KNN will typically do better if the distribution of predictors is non-Normal
####- KNN depends on value of K, so some K's may do better than others


#Validation Set Approach (what we've been doing so far)
####- Purpose is to estimate MSE for future data
####- Use this estimate to "tune" models and choose variables
```{r}
set.seed(123)
library(ISLR)
data <- Auto

#Split into training set and validation set
i.train <- sample(1:nrow(data), round(nrow(data)/2))
data.train <- data[i.train, ]
data.validate <- data[-i.train, ]

#Example with 2nd order polynomial. Can do this with many polynomial orders
m2 <- lm(mpg~poly(horsepower,2), data=data.train)

#Make predictions and compute MSE
predictions <- predict(m2, newdata=data.validate)
mse <- mean((predictions - data.validate$mpg)^2)
mse
```

####- Using less than all data results in more biased estimates of testing MSE
####- Increasing size of training data is the goal
####- Here is an approach that produces a low-bias estimate of future MSE


#LOOCV
####- Use as large as a training dataset as possible
####- Produces very low bias, but is a very imprecise estimate of testing MSE
####- Only works for LS regression
####- **Produces estimates that are highly correlated which has greater variability (not good for estimating future datasets) but low bias**
```{r}
n <- nrow(Auto)
mse <- c()

#polynomial order
m <- 2
for(i in 1:n){
  model <- lm(mpg~poly(horsepower, m), data = Auto[-i, ])
  
  prediction.i <- predict(model, newdata = data.frame(horsepower=Auto$horsepower[i]))
  
  mse[i] <- mean((prediction.i - Auto$horsepower[i])^2)
}

mse.test <- sum(mse)/n
mse.test
```

####- Refer to HW 3 last question for another LOOCV example


#K-Fold CV
```{r}
#Example with Logistic Regression (glm)
library(boot)
set.seed(4321)

k <- 10
cv.error <- c()
for(i in 1:k){
  glm.fit <- glm(mpg~poly(horsepower, i), data=Auto)
  
  test.error <- cv.glm(Auto, glm.fit, K=10)$delta[1]
  cv.error <- c(cv.error, test.error)
}

plot(cv.error~c(1:10), xlab = 'Polynomial Degree')
lines(cv.error~c(1:10))



#K-fold CV on KNN
bank<-read.delim("banknote.txt")
bank<-na.omit(bank)


library(class)
set.seed(123)
k = 10
n <- nrow(bank)
fold = sample(1:k, n, replace=T)
classification <- matrix(NA,nrow=10,ncol=10)
for(j in 1:k){
  train <- bank[fold !=j ,]
  test <- bank[fold == j,]
  for(i in 1:10){
    #Exclude Diagonal and Response columns
    k <- knn(train[,1:5], test[,1:5], cl=train$Y, k = i)
    classification[j,i]<-mean(k != test$Y)
  }
  
}
classification

avg_error <- 1:10
for(i in 1:10){
  avg_error[i] <- sum(classification[, i])/10
}
avg_error
which.min(avg_error)
##use k=4
```



#Feature Selection
####- Given a response variable, which subset of predictor variables is best for prediction?
####- Need 2 things:
####- A strategy for searching (**best subsets, forward stepwise, backward stepwise**)
####- A selection criteria (**adjusted R-squared, AIC, BIC, CP, CV**)

####- Best subsets considers all models and thus produces lowest RSS
####- Smaller AIC is better
####- CP rewards models that make MSE smaller but adds a penalty for models that add variables. It is also an unbiased estimate of the testing MSE. It uses a coefficient of 2 to penalize models. 
####- For LS regression, if errors are gaussian then AIC is proportional to CP so they will choose the same models (with smaller values)
####- BIC penalizes for larger sample sizes. Smaller BIC is better. It uses a coefficient of log(n) to penalize models.
####- CV makes the least amount of assumptions about the model in terms of the selection criterias
```{r}
library(leaps)
library(caret)
train.data <- read.csv('boston.train.csv')

test.data <- read.csv('boston.test.csv')


predict.regsubsets = function(object, newdata, id, ...) {
    form = as.formula(object$call[[2]])
    mat = model.matrix(form, newdata)
    coefi = coef(object, id = id)
    mat[, names(coefi)] %*% coefi
} #R has no predict function for regsubsets

#Regsubsets searching
#For Forwards and Backwards: specify "method" argument in regsubsets()
models <- regsubsets(crim~., data = train.data, nvmax = 14)
summary(models)

data.frame(
  adjusted_r2 = which.max(summary(models)$adjr2),
  CP = which.min(summary(models)$cp),
  BIC = which.min(summary(models)$bic)
)

coef(models, 8) #coefficients for model selected by CP criteria

prediction.cp <- predict.regsubsets(models, newdata=test.data, id=8)
mse.cp <- mean((prediction.cp - test.data$crim)^2)
mse.cp #MSE for model selected by CP criteria

coef(models, 3) #coefficients for model selected by BIC criteria

prediction.bic <- predict.regsubsets(models, newdata=test.data, id=3)
mse.bic <- mean((prediction.bic - test.data$crim)^2)
mse.bic #MSE for model selected by BIC criteria


#CV method for selection criteria
set.seed(123)
k <- 10
n <- nrow(train.data)
folds <- sample(1:k, n, replace = T)
cv.error.matrix <- matrix(NA,k,13)

train.data <- na.omit(train.data)
for(j in 1:k){
  best.fit <- regsubsets(crim~.,data=train.data[folds!=j,], nvmax=13)
  for(i in 1:13){
    prediction <- predict(best.fit, train.data[folds==j,], id = i)
    cv.error.matrix[j,i] = mean((train.data$crim[folds==j]-prediction)^2)
    
  }
}

average.mse <- apply(cv.error.matrix,2,mean)
which.min(average.mse) #Number of parameters chosen by 10-fold cv

coef(models, 9)

prediction.cv <- predict(models, newdata=test.data, id=9)
mse.cv <- mean((prediction.cv-test.data$crim)^2)
mse.cv 
```



#Model Selection Beyond Least Squares (but still linear)
#Ridge Regression
####- Adds penalty term lambda to LS
####- Variables approach 0
####- All variables are standardized
####- Variables with larger coefficients are considered more important
####- Ridge can have lower MSE than LS, but is less variable than LS (because high-variable parameters are shrunk)
####- No missing values
####- No text variables
####- Predictors in 1 matrix, response in a vector
####- As lambda gets bigger, coefficients get smaller really fast, variability decreases and bias increases
####- Variability decreases faster than bias increases
####- Optimal lambda minimizes MSE (we measure MSE here through CV)
####- Useful when p is large relative to n (i.e. helpful for smaller sample sizes)

```{r}
library(glmnet)
set.seed(123)

#Remove NA's
train.data <- na.omit(train.data)

#Remove response variable in train data
x <- model.matrix(crim~., data = train.data)[,-1]
y <- train.data$crim

cv.ridge <- cv.glmnet(x,y, alpha = 0)
plot(cv.ridge)

bestlam <- cv.ridge$lambda.min
bestlam

model.ridge <- glmnet(x,y,alpha = 0, lambda = bestlam)
coef(model.ridge)

#Remove response variable in test data
xtest <- as.matrix(test.data)[,-1]
prediction.ridge <- predict(model.ridge, newx=xtest)
mse.ridge <- mean((prediction.ridge-test.data$crim)^2)
mse.ridge
```



#LASSO Regression
####- Similar to ridge; uses the L1 norm
####- Forces some variables to 0 and is thus good for model selection
```{r}

cv.lasso <- cv.glmnet(x,y, alpha = 1)
plot(cv.lasso)
bestlam <- cv.lasso$lambda.min #lowest lambda
bestlam

model.lasso <- glmnet(x,y,alpha = 1, lambda = bestlam)
coef(model.lasso)

#Remove response variable in test data
xtest <- as.matrix(test.data)[,-1]
prediction.lasso <- predict(model.lasso, newx=xtest)
mse.lasso <- mean((prediction.lasso-test.data$crim)^2)
mse.lasso
```



#Principal Components Regression
####- Reduce dimensions of p predictor variables in order to reduce flexibility and improve MSE
####- No variables are dropped; Each m variable (where m < p) is a sum of all of the original p-variables
####- Choose principal components by choosing the variables that point in the direction of maximum variation (as to retain as much info as possible)
####- The # of principal components to keep is determined by which model produces the best testing MSE
####- Example of Unsupervised learning because the PCs are determined without reference to the response variable
```{r}
#Split into train and test
set.seed(12345)
library(ISLR)
library(pls)

mydata <- College

train <- sample(1:nrow(mydata), nrow(mydata)/2)
mydata.train <- mydata[train, ]
mydata.test <- mydata[-train, ]


#PCR
pcr.model <- pcr(Apps~., data = mydata.train, scale = TRUE, validation = 'CV')
summary(pcr.model) #Minimum at 17 on the plot

validationplot(pcr.model, val.type = 'MSEP')

#Remove response variable from test data
prediction.pcr <- predict(pcr.model, mydata.test[ ,-2], ncomp = 17) 
mse <- mean((prediction.pcr - mydata.test$Apps)^2)
mse
```



#Partial Least Squares
####- Supervised alternative to PCR
```{r}
#PLS
pls.model <- plsr(Apps~., data = mydata.train, scale = TRUE, validation = 'CV')
summary(pls.model) #Minimum at 10

validationplot(pls.model, val.type = 'MSEP')

#Remove response variable from test data
prediction.pls <- predict(pls.model, mydata.test[ ,-2], ncomp = 10) 
mse <- mean((prediction.pls - mydata.test$Apps)^2)
mse
```



#CART Classification and Regression Trees
####- Take a predictor x1 and sort the values so that x1[1] is the smallest
####- Split data into two groups (one group where observations are smaller than x1[1] and one group where observations are bigger than x1[1])
####- In each group (called a node), classify as A if majority are in group A or classify as B if majority are in group B
####- Measure "purity" of each split. "Purity" refers to how homogenous the group is. A small value indicates that all objects in a node are the same whereas a large value indicates that the split is more 50-50
####- Repeat above with x1[2]
####- Save best x1[i] with best purity
####- Repeat using x2, x3,....,xi
####- Stop when purity no longer improves or until each node has nmin observations in it

####- Purity measured by Gini index
####- Total purity for a tree is sum of purity of all terminal nodes

####- Trees are prone to overfitting. Prune to minimize overfitting but watch out for increase in misclassification error

####- Trees perform better than regression methods if boundaries aren't linear
```{r}
library(tree)
icu.cleaned <- read.csv("icu.cleaned.csv", sep = "\t")
set.seed(123)

i.train <- sample(c(TRUE, FALSE), 200, replace = TRUE)
icu.train <- icu.cleaned[i.train, ]
icu.test <- icu.cleaned[-i.train, ]

#Fit a full tree predicting STA (survived or died in ICU)
model.tree <- tree(STA~., data = icu.train)
summary(model.tree)
plot(model.tree)
text(model.tree, pretty = TRUE)

#Make predictions on testing data
prediction.tree <- predict(model.tree, newdata = icu.test, type = "class")

table(prediction.tree, icu.test$STA)
mean(prediction.tree != icu.test$STA)
```

Our tree has a misclassification rate of 14% (note this is highly variable from tree to tree).

Prune the tree to minimize overfitting
```{r}
#Cross Validation to determine optimal level of tree complexity
cv.icu <- cv.tree(model.tree, FUN = prune.misclass)
plot(cv.icu$dev ~ cv.icu$size)
#best size appears to be 1 (therefore no pruning needed). Command does not work with 1 however so use 2 for this example

pruned.fit <- prune.misclass(model.tree, best = 2)

#Make predictions on testing data
prediction.tree <- predict(pruned.fit, newdata = icu.test, type = "class") 
#type = "class" gives us predicted class probabilities. type = "vector" gives us raw probabilities

table(prediction.tree, icu.test$STA)
mean(prediction.tree != icu.test$STA)
```

Test MSE is slightly worse when we use a non-optimal size (2 in this case)
####- Optimal way of going about this is restricting variability of trees in order to improve test MSE, without lowering bias
####- Solution: use bootstrapping (implemented through bagging)


#Bagging
####- Fit unpruned tree with initial sample
####- Take a sample of size n (with replacement) from the original sample and fit an unpruned tree
####- Repeat around 500 times, so you will have 500 trees
####- For each observation in initial sample, drop it down each of the 500 trees
####- Each tree will "vote" on the classification of the observation
####- Majority vote wins
####- Some observations will be used several times while others may not appear at all
####- Averaging improves our predictions by decreasing variability while maintaining low bias of a tree
```{r}
library(randomForest)
set.seed(123)

bagging <- randomForest(STA~., data = icu.cleaned, mtry=19, importance = TRUE)
print(bagging)
```

####- Printing the model provides an estimation of misclassification rate (pretty handy)

####- Problem with bagging: an influential variable may cause the trees to vote in the same way, causing the trees to be "correlated" (similar to LOOCV)

####- Solution: use random forests


#Random Forests
####- How is this different from the above?
####- When splitting the nodes, instead of going through all possible splits for all possible variables, go through all possible splits on a random sample of a small number of variables m, where m < p
####- Bagging is where m = p
####- This works because variables that are strongly correlated will get a chance to appear by themselves without other potentially correlated variables
####- Final product is a black box which means it is hard to interpret the model
```{r}
model.rf <- randomForest(STA~., data = icu.cleaned, mtry = 4, importance = TRUE) #mtry specifies the number of variables to use.
#importance specifies whether or not to calculate the relative importance of each variable used (will slow down the model)
print(model.rf)

#Plot
plot(model.rf)

#Variable importance
varImpPlot(model.rf)
```

####- Fine tuning for random forests
```{r}
forest2 <- randomForest(STA~.,data=icu.cleaned,mtry=4,cutoff=c(.4,.6))
forest2
#Change the cutoff for tree to vote on whether a patient "dies" to 40% 
#So if atleast 40% of trees vote die, then we classify as die


forest3 <- randomForest(STA~.,data=icu.cleaned,mtry=4,sampsize=c(40,100))
#Change the sample size in boostrap and control it by outcome category
#In our data, 40 die and 160 survive
#By setting to c(40, 100), we are getting a greater percentage from "die" on average
#Can also take in a single integer argument
```


#Boosting
####- An improvement over bagging
####- Still grow multiple trees, except each subsequent tree depends on the previous tree using "slow learning"
####- Next tree is created with the residuals (leftover information that could not be explained) of the previous tree
```{r}
library(gbm)
library(dplyr)
set.seed(123)
pgatour2006 <- read.csv("pgatour2006.csv")


pga <- mutate(pgatour2006,lprize = log(PrizeMoney)) %>%
select(TigerWoods, AveDrivingDistance:lprize)

fit.gbm <- gbm(lprize~.,data=pga,
interaction.depth=2,n.trees=500,shrinkage=.001,
distribution="gaussian")
summary(fit.gbm)
#rel.inf gives the relative influence of each variable (higher value means higher influence)

plot(fit.gbm, i.var = "GIR")
#We see that GIR is very important in the range of 62 - 65
```



#Support Vector Machines
####- Choose a boundary so that the margin between the two groups of points is as large as possible
####- Based on kernels that control the shape of the classifying boundary
####- Can set parameters to use a linear kernel or radial kernel
####- Radial kernels require gamma parameter. A large value of gamma is less flexible and a small value of gamma is more flexible
####- Linear kernels require the degree of the polynomial
```{r}
library(e1071)
set.seed(123)

#SVM model
svmfit <- svm(STA~.,data=icu.train,kernel="radial", gamma=1)

#Calculate predictions using test data
prediction.svmfit <- predict(svmfit,newdata=icu.test)
table(prediction.svmfit,icu.test$STA)
mean(prediction.svmfit != icu.test$STA)

#Tune the model
tune.out <- tune(svm,STA~.,data=icu.train,kernel="radial", ranges=list(cost=c(.1,1,10,100,1000), gamma=c(0.4,1,2,3,4)))
tune.out$best.parameters

#Predictions with best-tuned model
prediction.best <- predict(tune.out$best.model,newdata=icu.test)
table(prediction.best,icu.test$STA)
```


#Splines
####- A class of very flexible, non-linear models
####- Created by a combination of polynomial regression (global) and step functions (local)
####- Increase flexibility in the hopes of reducing bias enough to improve testing MSE
####- Regression splines (linear, cubic, natural) and Smoothing splines
####- Degree splines are constrained to be continuous and smooth (aka first and second derivatives exist), but can have very high variability at the outer ranges
####- Natural splines solve this problem while maintaining everything previous by adding linear constraints at the outer ranges
```{r}
library(splines)

re2 <- read.csv("realestate.csv")

#Fit the spline with 3 knots
ns.fit=lm(price~ns(sqft,knots=c(3000,5000,10000)),data=re2) 

minimum <- min(re2$sqft, na.rm = T)
maximum <- max(re2$sqft, na.rm = T)
sqft.grid <- seq(minimum,maximum)

pred=predict(ns.fit,newdata=list(sqft=sqft.grid),se=T) 

plot(price~sqft,data=re2)

lines(sqft.grid,pred$fit, col="red")
```





